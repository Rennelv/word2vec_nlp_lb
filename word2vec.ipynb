{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def embeddings_path(dataset_name, embedding_size, bpe=False):\n",
    "    if bpe:\n",
    "        return f\"embeddings/{dataset_name}_embeddings_{embedding_size}_bpe.txt\"\n",
    "    return f\"embeddings/{dataset_name}_embeddings_{embedding_size}.txt\"\n",
    "\n",
    "def tokenize_text(input_file, output_file, tokenizer, regex, new_line_marker='\\n', max_sentences=None):\n",
    "    sentences = []\n",
    "    with open(input_file, 'r', encoding='utf-8') as fin, \\\n",
    "         open(output_file, 'w', encoding='utf-8') as fout:\n",
    "        pattern = re.compile(regex, re.UNICODE)\n",
    "        for line in fin:\n",
    "            tokens = pattern.findall(line.lower())\n",
    "            if not tokens:\n",
    "                continue\n",
    "            if tokenizer == 'word':\n",
    "                sentences.append(tokens)\n",
    "                sentences[-1].append(new_line_marker)\n",
    "            elif tokenizer == 'char':\n",
    "                words = []\n",
    "                for token in tokens:\n",
    "                    token_chars = list(token)\n",
    "                    token_chars.append('</w>')\n",
    "                    tokenized_word = ' '.join(token_chars)\n",
    "                    words.append(tokenized_word)\n",
    "                sentences.append(words)\n",
    "                sentences[-1].append(new_line_marker)\n",
    "            fout.write(\" \".join(sentences[-1]) + ' ')\n",
    "            if max_sentences is not None and len(sentences) >= max_sentences:\n",
    "                break\n",
    "    return sentences\n",
    "\n",
    "def load_sentences(file_path):\n",
    "    sentences = []\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            tokens = line.strip().split()\n",
    "            sentences.append(tokens)\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words: 6000812\n",
      "Total number of sentences: 1\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"datasets/harrypotter.txt\"\n",
    "# dataset_path = \"datasets/chan_dialogues.txt\"\n",
    "# dataset_path = \"datasets/war.txt\"\n",
    "dataset_name = dataset_path.split('/')[-1].split('.')[0]\n",
    "\n",
    "embeddings_d = [100, 500, 1000]\n",
    "max_vocab_size = 120000\n",
    "\n",
    "max_sentences = None\n",
    "\n",
    "bpe = True\n",
    "\n",
    "# pattern = r'\\w+|[^\\w\\s]' # со знаками препинания\n",
    "pattern =  r\"\\w+\"        # только слова   \n",
    "tokenizer = 'char' if bpe else 'word'\n",
    "tokenize_text(dataset_path, f\"sentences/{dataset_name}_sentences_{tokenizer}.txt\", tokenizer=tokenizer, regex=pattern, new_line_marker='</n>', max_sentences=max_sentences)\n",
    "sentences = load_sentences(f\"sentences/{dataset_name}_sentences_{tokenizer}.txt\")\n",
    "\n",
    "total_words = sum(len(sentence) for sentence in sentences)\n",
    "print(f\"Total number of words: {total_words}\")\n",
    "print(f\"Total number of sentences: {len(sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Прочитано 1 строк. (0.04712 сек)\n",
      "Итерация 1: найдено 5141 уникальных пар. (1.47653 сек)\n",
      "Лучшая пара: о + </w> -> о</w>\n",
      "Слияние 1 завершено. (1.84997 сек)\n",
      "Итерация 101: найдено 21519 уникальных пар. (89.0652 сек)\n",
      "Лучшая пара: б + ы -> бы\n",
      "Слияние 101 завершено. (89.2367 сек)\n",
      "Итерация 201: найдено 44529 уникальных пар. (167.161 сек)\n",
      "Лучшая пара: ако + й</w> -> акой</w>\n",
      "Слияние 201 завершено. (167.277 сек)\n",
      "Итерация 301: найдено 72396 уникальных пар. (252.666 сек)\n",
      "Лучшая пара: г + е -> ге\n",
      "Слияние 301 завершено. (252.779 сек)\n",
      "Итерация 401: найдено 102418 уникальных пар. (328.059 сек)\n",
      "Лучшая пара: хо + ди -> ходи\n",
      "Слияние 401 завершено. (328.186 сек)\n",
      "Итерация 501: найдено 132135 уникальных пар. (417.125 сек)\n",
      "Лучшая пара: ю + сь</w> -> юсь</w>\n",
      "Слияние 501 завершено. (417.246 сек)\n",
      "Итерация 601: найдено 161600 уникальных пар. (525.338 сек)\n",
      "Лучшая пара: ве + н -> вен\n",
      "Слияние 601 завершено. (525.435 сек)\n",
      "Итерация 701: найдено 188559 уникальных пар. (612.089 сек)\n",
      "Лучшая пара: пос + лед -> послед\n",
      "Слияние 701 завершено. (612.178 сек)\n",
      "Итерация 801: найдено 213463 уникальных пар. (717.673 сек)\n",
      "Лучшая пара: ж + о -> жо\n",
      "Слияние 801 завершено. (717.775 сек)\n",
      "Итерация 901: найдено 238088 уникальных пар. (821.73 сек)\n",
      "Лучшая пара: ти + ть</w> -> тить</w>\n",
      "Слияние 901 завершено. (821.845 сек)\n",
      "Итерация 1001: найдено 261815 уникальных пар. (931.45 сек)\n",
      "Лучшая пара: ано + ны</w> -> аноны</w>\n",
      "Слияние 1001 завершено. (931.55 сек)\n",
      "Итерация 1101: найдено 285449 уникальных пар. (1047.34 сек)\n",
      "Лучшая пара: пиз + до -> пиздо\n",
      "Слияние 1101 завершено. (1047.45 сек)\n",
      "Итерация 1201: найдено 306237 уникальных пар. (1175.94 сек)\n",
      "Лучшая пара: бе + з -> без\n",
      "Слияние 1201 завершено. (1176.11 сек)\n",
      "Итерация 1301: найдено 326665 уникальных пар. (1308.33 сек)\n",
      "Лучшая пара: чита + л</w> -> читал</w>\n",
      "Слияние 1301 завершено. (1308.42 сек)\n",
      "Итерация 1401: найдено 345438 уникальных пар. (1419.25 сек)\n",
      "Лучшая пара: музы + ки</w> -> музыки</w>\n",
      "Слияние 1401 завершено. (1419.42 сек)\n",
      "Итерация 1501: найдено 363208 уникальных пар. (1593.14 сек)\n",
      "Лучшая пара: в + не -> вне\n",
      "Слияние 1501 завершено. (1593.28 сек)\n",
      "Итерация 1601: найдено 380191 уникальных пар. (1744.78 сек)\n",
      "Лучшая пара: друго + й</w> -> другой</w>\n",
      "Слияние 1601 завершено. (1744.9 сек)\n",
      "Итерация 1701: найдено 396585 уникальных пар. (1846.8 сек)\n",
      "Лучшая пара: каки + х</w> -> каких</w>\n",
      "Слияние 1701 завершено. (1846.89 сек)\n",
      "Итерация 1801: найдено 412586 уникальных пар. (1925.24 сек)\n",
      "Лучшая пара: ви + л</w> -> вил</w>\n",
      "Слияние 1801 завершено. (1925.33 сек)\n",
      "Итерация 1901: найдено 427781 уникальных пар. (2003.84 сек)\n",
      "Лучшая пара: ани + ме -> аниме\n",
      "Слияние 1901 завершено. (2003.93 сек)\n",
      "Итерация 2001: найдено 441637 уникальных пар. (2083.29 сек)\n",
      "Лучшая пара: 1 + 9</w> -> 19</w>\n",
      "Слияние 2001 завершено. (2083.37 сек)\n",
      "Итерация 2101: найдено 454745 уникальных пар. (2163.68 сек)\n",
      "Лучшая пара: мо + м</w> -> мом</w>\n",
      "Слияние 2101 завершено. (2163.77 сек)\n",
      "Итерация 2201: найдено 467878 уникальных пар. (2246.66 сек)\n",
      "Лучшая пара: дол + жно</w> -> должно</w>\n",
      "Слияние 2201 завершено. (2246.75 сек)\n",
      "Итерация 2301: найдено 479508 уникальных пар. (2339.95 сек)\n",
      "Лучшая пара: мо + л -> мол\n",
      "Слияние 2301 завершено. (2340.04 сек)\n",
      "Итерация 2401: найдено 491019 уникальных пар. (2425.66 сек)\n",
      "Лучшая пара: раз + бога -> разбога\n",
      "Слияние 2401 завершено. (2425.75 сек)\n",
      "Итерация 2501: найдено 502157 уникальных пар. (2515.56 сек)\n",
      "Лучшая пара: разбога + теть</w> -> разбогатеть</w>\n",
      "Слияние 2501 завершено. (2515.68 сек)\n",
      "Итерация 2601: найдено 511870 уникальных пар. (2609.21 сек)\n",
      "Лучшая пара: i + t -> it\n",
      "Слияние 2601 завершено. (2609.29 сек)\n",
      "Итерация 2701: найдено 521958 уникальных пар. (2715.64 сек)\n",
      "Лучшая пара: майо + р</w> -> майор</w>\n",
      "Слияние 2701 завершено. (2715.76 сек)\n",
      "Итерация 2801: найдено 531553 уникальных пар. (2865.78 сек)\n",
      "Лучшая пара: яй + ца</w> -> яйца</w>\n",
      "Слияние 2801 завершено. (2865.9 сек)\n",
      "Итерация 2901: найдено 540747 уникальных пар. (3011.12 сек)\n",
      "Лучшая пара: ра + шке</w> -> рашке</w>\n",
      "Слияние 2901 завершено. (3011.25 сек)\n",
      "BPE-токенизация завершена. (3143.56 сек)\n",
      "BPE-токенизация завершена. Результат сохранён в sentences/chan_dialogues_sentences_bpe.txt (3143.59 сек)\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "if bpe:\n",
    "    subprocess.run([\"./bpe_tokenizer\",\n",
    "                    f\"sentences/{dataset_name}_sentences_{tokenizer}.txt\",\n",
    "                    f\"sentences/{dataset_name}_sentences_bpe.txt\",\n",
    "                   '3000'\n",
    "                   ], check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words after BPE: 1421051\n"
     ]
    }
   ],
   "source": [
    "if bpe:\n",
    "    sentences = load_sentences(f\"sentences/{dataset_name}_sentences_bpe.txt\")\n",
    "    total_words = sum(len(sentence) for sentence in sentences)\n",
    "    print(f\"Total number of words after BPE: {total_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст: sentences/harrypotter_sentences_bpe.txt\n",
      "Эмбеддинги: embeddings/harrypotter_embeddings_100_bpe.txt\n",
      "Размер эмбеддингов (d): 100\n",
      "Макс. размер словаря: 120000\n",
      "Размер окна: 5\n",
      "Количество отрицательных примеров: 5\n",
      "Количество эпох: 5\n",
      "Скорость обучения (lr): 0.025\n",
      "\n",
      "Загрузка текста и создание словаря...\n",
      "Текст прочитан. (0.127667 сек)\n",
      "Количество строк: 1\n",
      "Количество уникальных слов: 3011\n",
      "\n",
      "Словарь создан. (0.128381 сек)\n",
      "Размер словаря: 3011\n",
      "Макс. размер словаря: 120000\n",
      "Примеры словаря: \n",
      "the</w> (52269) -> 0; </n> (38195) -> 1; and</w> (28090) -> 2; to</w> (27261) -> 3; he</w> (22343) -> 4; a</w> (22282) -> 5; of</w> (22076) -> 6; harry</w> (18367) -> 7; s</w> (16007) -> 8; was</w> (15691) -> 9; \n",
      "\n",
      "Предложения закодированы. (0.172589 сек)\n",
      "Количество закодированных предложений: 1\n",
      "Примеры закодированных предложений: \n",
      "65 1656 2 269 2403 6 1069 748 2425 1846 \n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "for d in embeddings_d:\n",
    "    window_size = 5\n",
    "    num_negative_examples = 5\n",
    "    num_epoch = 5 + d // 500\n",
    "    learning_rate = 0.025\n",
    "    subprocess.run([\"./generate_embeddings\",            # executable\n",
    "                    f\"sentences/{dataset_name}_sentences_{'bpe' if bpe else 'word'}.txt\",       # path to tokenized sentences\n",
    "                    embeddings_path(dataset_name, d, bpe=bpe),   # path to save embeddings to\n",
    "                    str(d),                             # word vec dim\n",
    "                    str(max_vocab_size),                # max size of vocabulary\n",
    "                    str(window_size),                                # windows size\n",
    "                    str(num_negative_examples),                                # num negative examples\n",
    "                    str(num_epoch),                                # num epoch\n",
    "                    str(learning_rate),                             # learning rate\n",
    "                    ], check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_embeddings(file_path):\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        header = f.readline().strip().split()\n",
    "        vocab_size = int(header[0])\n",
    "        d = int(header[1])\n",
    "        embeddings = np.zeros((vocab_size, d), dtype=np.float32)\n",
    "        word2index = {}\n",
    "        index2word = []\n",
    "        for i, line in enumerate(f):\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) != d + 1:\n",
    "                continue \n",
    "            word = parts[0]\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            embeddings[i] = vector\n",
    "            word2index[word] = i\n",
    "            index2word.append(word)\n",
    "    return embeddings, word2index, index2word\n",
    "\n",
    "# embeddings_file = embeddings_path(dataset_name, 100)  \n",
    "# embeddings, word2index, index2word = load_embeddings(embeddings_file)\n",
    "# print(\"Загруженная матрица эмбеддингов:\", embeddings.shape)\n",
    "# print(\"Примеры слов:\", list(word2index.keys())[:10])\n",
    "\n",
    "# print(\"Примеры векторов:\", list(embeddings)[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_examples_from_sentence(sentence, L):\n",
    "    \"\"\"\n",
    "    Генерирует обучающие примеры (контекст, целевой токен) из предложения.\n",
    "    где контекст — это L последовательных токенов, а целевой токен — следующий за ними.\n",
    "    \"\"\"\n",
    "    if len(sentence) < L + 1:\n",
    "        return  # Недостаточно токенов для формирования хотя бы одного примера.\n",
    "    for i in range(len(sentence) - L):\n",
    "        context = sentence[i : i + L]\n",
    "        target = sentence[i + L]\n",
    "        yield context, target\n",
    "        \n",
    "def batch_generator_pretokenized(tokenized_sentences, word2index, L, batch_size=512):\n",
    "    \"\"\"\n",
    "    Генератор батчей обучающих примеров из токенизированных предложений\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        X, y = [], []\n",
    "        for sentence in tokenized_sentences:\n",
    "            for context, target in generate_training_examples_from_sentence(sentence, L):\n",
    "                X.append([word2index.get(w, 0) for w in context])\n",
    "                y.append(word2index.get(target, 0))\n",
    "                if len(X) >= batch_size:\n",
    "                    yield np.array(X), np.array(y)\n",
    "                    X, y = [], []  # Очищаем батч\n",
    "        if X:  # Возвращаем остаток батча\n",
    "            yield np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 19:21:42.972363: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742660502.981586  646340 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742660502.984007  646340 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742660502.992047  646340 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742660502.992059  646340 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742660502.992061  646340 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742660502.992062  646340 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-22 19:21:42.994939: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер словаря: 3011, размер эмбеддинга: 100\n",
      "Всего слов: 1421051, шагов на эпоху: 2776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rennelv/etu/ai/.conda/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "I0000 00:00:1742660507.119130  646340 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2152 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"word_prediction_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"word_prediction_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ pretrained_embedding            │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">301,100</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_hidden (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output_softmax (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ pretrained_embedding            │ ?                      │       \u001b[38;5;34m301,100\u001b[0m │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)                     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_layer (\u001b[38;5;33mLSTM\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_hidden (\u001b[38;5;33mDense\u001b[0m)            │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output_softmax (\u001b[38;5;33mDense\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">301,100</span> (1.15 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m301,100\u001b[0m (1.15 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">301,100</span> (1.15 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m301,100\u001b[0m (1.15 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742660509.213132  646756 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8ms/step - accuracy: 0.0713 - loss: 6.0672\n",
      "Epoch 2/10\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 8ms/step - accuracy: 0.1565 - loss: 4.9612\n",
      "Epoch 3/10\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 8ms/step - accuracy: 0.1926 - loss: 4.6015\n",
      "Epoch 4/10\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 8ms/step - accuracy: 0.2106 - loss: 4.4137\n",
      "Epoch 5/10\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 8ms/step - accuracy: 0.2218 - loss: 4.2932\n",
      "Epoch 6/10\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 8ms/step - accuracy: 0.2297 - loss: 4.2055\n",
      "Epoch 7/10\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 8ms/step - accuracy: 0.2365 - loss: 4.1371\n",
      "Epoch 8/10\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 8ms/step - accuracy: 0.2417 - loss: 4.0818\n",
      "Epoch 9/10\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 8ms/step - accuracy: 0.2461 - loss: 4.0359\n",
      "Epoch 10/10\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 8ms/step - accuracy: 0.2497 - loss: 3.9967\n",
      "Размер словаря: 3011, размер эмбеддинга: 500\n",
      "Всего слов: 1421051, шагов на эпоху: 2776\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"word_prediction_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"word_prediction_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ pretrained_embedding            │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,505,500</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_hidden (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output_softmax (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ pretrained_embedding            │ ?                      │     \u001b[38;5;34m1,505,500\u001b[0m │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)                     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_layer (\u001b[38;5;33mLSTM\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_hidden (\u001b[38;5;33mDense\u001b[0m)            │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output_softmax (\u001b[38;5;33mDense\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,505,500</span> (5.74 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,505,500\u001b[0m (5.74 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,505,500</span> (5.74 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,505,500\u001b[0m (5.74 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - accuracy: 0.0753 - loss: 6.0179\n",
      "Epoch 2/10\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 9ms/step - accuracy: 0.1695 - loss: 4.8281\n",
      "Epoch 3/10\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 9ms/step - accuracy: 0.2026 - loss: 4.4889\n",
      "Epoch 4/10\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 9ms/step - accuracy: 0.2191 - loss: 4.3189\n",
      "Epoch 5/10\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 9ms/step - accuracy: 0.2293 - loss: 4.2062\n",
      "Epoch 6/10\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - accuracy: 0.2369 - loss: 4.1231\n",
      "Epoch 7/10\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 9ms/step - accuracy: 0.2432 - loss: 4.0575\n",
      "Epoch 8/10\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 9ms/step - accuracy: 0.2483 - loss: 4.0044\n",
      "Epoch 9/10\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - accuracy: 0.2522 - loss: 3.9597\n",
      "Epoch 10/10\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - accuracy: 0.2558 - loss: 3.9216\n",
      "Размер словаря: 3011, размер эмбеддинга: 1000\n",
      "Всего слов: 1421051, шагов на эпоху: 2776\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"word_prediction_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"word_prediction_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ pretrained_embedding            │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,011,000</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_hidden (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output_softmax (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ pretrained_embedding            │ ?                      │     \u001b[38;5;34m3,011,000\u001b[0m │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)                     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_layer (\u001b[38;5;33mLSTM\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_hidden (\u001b[38;5;33mDense\u001b[0m)            │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output_softmax (\u001b[38;5;33mDense\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,011,000</span> (11.49 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,011,000\u001b[0m (11.49 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,011,000</span> (11.49 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,011,000\u001b[0m (11.49 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10ms/step - accuracy: 0.0794 - loss: 5.9705\n",
      "Epoch 2/10\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 10ms/step - accuracy: 0.1761 - loss: 4.7491\n",
      "Epoch 3/10\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 10ms/step - accuracy: 0.2064 - loss: 4.4363\n",
      "Epoch 4/10\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 10ms/step - accuracy: 0.2225 - loss: 4.2746\n",
      "Epoch 5/10\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 10ms/step - accuracy: 0.2326 - loss: 4.1667\n",
      "Epoch 6/10\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10ms/step - accuracy: 0.2404 - loss: 4.0862\n",
      "Epoch 7/10\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 10ms/step - accuracy: 0.2462 - loss: 4.0231\n",
      "Epoch 8/10\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 10ms/step - accuracy: 0.2511 - loss: 3.9714\n",
      "Epoch 9/10\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 10ms/step - accuracy: 0.2554 - loss: 3.9278\n",
      "Epoch 10/10\n",
      "\u001b[1m2776/2776\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10ms/step - accuracy: 0.2592 - loss: 3.8905\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.backend import clear_session as clear_keras_session\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "L = 5    # Длина контекста для предсказания\n",
    "lstm_units = 128\n",
    "dh = 512  # Размер скрытого слоя\n",
    "epochs = 10\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "steps_per_epoch = np.ceil(total_words / batch_size).astype(int)\n",
    "\n",
    "for d in embeddings_d:\n",
    "    embeddings_file = embeddings_path(dataset_name, d, bpe=bpe)\n",
    "    embeddings, word2index, index2word = load_embeddings(embeddings_file)\n",
    "\n",
    "    vocab_size = len(word2index)\n",
    "\n",
    "    print(f\"Размер словаря: {vocab_size}, размер эмбеддинга: {d}\")\n",
    "    print(f\"Всего слов: {total_words}, шагов на эпоху: {steps_per_epoch}\")\n",
    "\n",
    "    clear_keras_session()  # Очистка сессии Keras\n",
    "\n",
    "    # Создаем модель\n",
    "    \n",
    "#     model = models.Sequential(name=\"word_prediction_model\")\n",
    "#     model.add(layers.Embedding(input_dim=vocab_size, \n",
    "#                                 output_dim=d, \n",
    "#                                 weights=[embeddings], \n",
    "#                                 trainable=False, \n",
    "#                                 input_length=L, \n",
    "#                                 name=\"pretrained_embedding\"))\n",
    "#     model.add(layers.LSTM(lstm_units, name=\"lstm_layer\"))\n",
    "#     model.add(layers.Dense(dh, activation='relu', name=\"dense_hidden\"))\n",
    "#     model.add(layers.Dense(vocab_size, activation='softmax', name=\"output_softmax\"))\n",
    "\n",
    "    inputs = layers.Input(shape=(L,), name=\"context_input\")\n",
    "    embedding_layer = layers.Embedding(input_dim=vocab_size, \n",
    "                                       output_dim=d, \n",
    "                                       weights=[embeddings], \n",
    "                                       input_length=L, \n",
    "                                       trainable=False, \n",
    "                                       name=\"pretrained_embedding\")(inputs)\n",
    "    lstm_output = layers.LSTM(lstm_units, name=\"lstm_layer\")(embedding_layer)\n",
    "    hidden = layers.Dense(dh, activation='relu', name=\"dense_hidden\")(lstm_output)\n",
    "    outputs = layers.Dense(vocab_size, activation='softmax', name=\"output_softmax\")(hidden)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name=\"word_prediction_model\")\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    # Обучаем модель предсказания\n",
    "    model.fit(batch_generator_pretokenized(sentences, word2index, L, batch_size), \n",
    "            epochs=epochs, \n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            callbacks=[EarlyStopping(monitor='loss', patience=3)])\n",
    "\n",
    "    # Сохраняем модель\n",
    "    model.save(f\"models/{dataset_name}_model_{d}_{'bpe' if bpe else 'word'}.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тестовое предложение: 'the  only  way  to  sort  this  out  </n> i  m  not  running  around  after  him  trying '\n",
      "Тестовые индексы: [1120, 87, 187, 22, 329]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\n",
      "[100] Предсказанное следующее слово для 'running around after him trying ': to</w>\n",
      "[100] Перплексия на тестовом предложении: 60.64\n",
      "Сгенерированные слова:  to fight the sword of gryffindor s sword and the elder wand and the snake s eyes were fixed upon the ground and the snake s head was still holding the wand of the death eaters were now standing in the middle of the forest and the snake s eyes were fixed upon the ground and the snake s head was still holding the wand of the death eaters were now standing in the middle of the forest and the snake s eyes were fixed upon the ground and the snake s head was still holding the wand of \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
      "[500] Предсказанное следующее слово для 'running around after him trying ': to</w>\n",
      "[500] Перплексия на тестовом предложении: 74.78\n",
      "Сгенерированные слова:  to keep talking about the death eaters were forced to kill you </n>i think i was going to be able to do with the wand </n>i m sorry i m sorry i m sorry i m sorry i m sorry i m sorry i m sorry i m sorry i m sorry i m sorry i m sorry i m sorry i m sorry i m sorry i m sorry i m sorry i m sorry i m sorry i m sorry i m sorry i m sorry i m sorry i m sorry i m sorry i \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
      "[1000] Предсказанное следующее слово для 'running around after him trying ': to</w>\n",
      "[1000] Перплексия на тестовом предложении: 51.08\n",
      "Сгенерированные слова:  to save the boy who lived in the forest </n>the castle was the only one who had been killed by the time he had been killed by the time he had been killed by the time he had been killed by the time he had been killed by the time he had been killed by the time he had been killed by the time he had been killed by the time he had been killed by the time he had been killed by the time he had been killed by the time he had been killed by the time \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.backend import clear_session as clear_keras_session\n",
    "import random\n",
    "\n",
    "def evaluate_perplexity(model, test_sentence, L, word2index):\n",
    "    \"\"\"\n",
    "    Для каждого окна длины L модель предсказывает вероятность истинного следующего слова\n",
    "    perplexity = exp( -(1/N) * sum(log(P(w_i|context)))\n",
    "    где N - количество предсказаний, а P(w_i|context) - вероятность истинного слова\n",
    "    \"\"\"\n",
    "    total_log_prob = 0.0\n",
    "    count = 0\n",
    "\n",
    "    test_tokens = test_sentence.lower().split()\n",
    "    test_indices = [word2index.get(word, 0) for word in test_tokens]\n",
    "        \n",
    "    # Если тестовая последовательность слишком короткая возвращаем бесконечность\n",
    "    if len(test_indices) <= L:\n",
    "        return float('inf')\n",
    "\n",
    "    # Проходим по тестовой последовательности, формируя окна длины L\n",
    "    for i in range(len(test_indices) - L):\n",
    "        context = test_indices[i:i+L]         # Контекстное окно из L токенов\n",
    "        true_word = test_indices[i+L]           # Истинное следующее слово\n",
    "        \n",
    "        # Получаем распределение вероятностей для следующего слова\n",
    "        pred = model.predict(np.array([context]), verbose=0)[0]\n",
    "        # Берем вероятность истинного слова; добавляем маленькую константу для стабильности log\n",
    "        prob = pred[true_word] + 1e-10\n",
    "        \n",
    "        # Суммируем логарифмы вероятностей\n",
    "        total_log_prob += np.log(prob)\n",
    "        count += 1\n",
    "\n",
    "    if count == 0:\n",
    "        return float('inf')\n",
    "        \n",
    "    # Вычисляем среднее логарифмическое значение\n",
    "    avg_log_prob = total_log_prob / count\n",
    "    # Перплексия – экспонента от отрицательной средней логарифмической вероятности\n",
    "    perplexity = np.exp(-avg_log_prob)\n",
    "    return perplexity\n",
    "\n",
    "def tokenize_bpe(sentence, word2index):\n",
    "    \"\"\"\n",
    "    Токенизирует предложение на токены, используя словарь word2index.\n",
    "    Перебирает все возможные срезы для корректного разбиения.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for word in sentence.lower().split():\n",
    "        word = word + '</w>'  # Добавляем маркер конца слова\n",
    "        start = 0\n",
    "        while start < len(word):\n",
    "            for end in range(len(word), start, -1):  # Перебираем все возможные срезы\n",
    "                subword = word[start:end]\n",
    "                if subword in word2index:\n",
    "                    tokens.append(subword)\n",
    "                    start = end - 1  # Сдвигаем начало на конец найденного токена\n",
    "                    break\n",
    "            else:\n",
    "                # Если ни один токен не найден, добавляем символ как отдельный токен\n",
    "                tokens.append(word[start])\n",
    "            start += 1\n",
    "    return tokens\n",
    "\n",
    "# Длина контекстного окна для предсказания следующего слова\n",
    "L = 5  \n",
    "# Длина тестового предложения\n",
    "S = 15\n",
    "# Формируем один длинный список из всех слов текста\n",
    "sentences_unpacked = []\n",
    "for sentence in load_sentences(f\"sentences/{dataset_name}_sentences_{'bpe' if bpe else 'word'}.txt\"):\n",
    "    sentences_unpacked.extend(sentence)\n",
    "\n",
    "embeddings_file = embeddings_path(dataset_name, 100, bpe=bpe)\n",
    "_, word2index, index2word = load_embeddings(embeddings_file)\n",
    "\n",
    "# test_sentence = 'my name is harry'\n",
    "# bpe_tokens = tokenize_bpe(test_sentence, word2index)\n",
    "\n",
    "# print(f\"Токены BPE: {bpe_tokens}\")\n",
    "\n",
    "# Выбираем случайное окно в общем списке\n",
    "i = random.randint(0, len(sentences_unpacked) - S - 1)\n",
    "# Берем окно из S+1 слов\n",
    "test_sentence = \" \".join(sentences_unpacked[i:i+S+1])\n",
    "\n",
    "# Преобразуем предложение в список токенов (слова приводятся к нижнему регистру)\n",
    "test_tokens = test_sentence.lower().split()\n",
    "\n",
    "# Если число токенов меньше L, дополняем их пробелами; если больше, берем последние L токенов.\n",
    "if len(test_tokens) < L:\n",
    "    test_tokens = [' '] * (L - len(test_tokens)) + test_tokens\n",
    "elif len(test_tokens) > L:\n",
    "    test_tokens = test_tokens[-L:]\n",
    "\n",
    "print(f\"Тестовое предложение: '{test_sentence.replace('</w>', ' ')}'\")\n",
    "\n",
    "# Преобразуем токены в индексы\n",
    "test_indices = [word2index.get(word, 0) for word in test_tokens]\n",
    "print(f\"Тестовые индексы: {test_indices}\")\n",
    "\n",
    "# Для каждого размера эмбеддингов оцениваем модель\n",
    "for d in embeddings_d:\n",
    "    # Загружаем модель, соответствующую текущей размерности эмбеддингов\n",
    "    clear_keras_session()  # Очистка сессии Keras\n",
    "    model = models.load_model(f\"models/{dataset_name}_model_{d}_{'bpe' if bpe else 'word'}.keras\")\n",
    "\n",
    "    # Предсказываем следующее слово по текущему контексту\n",
    "    predicted = model.predict(np.array([test_indices]))\n",
    "    predicted_index = np.argmax(predicted)\n",
    "    predicted_word = index2word[predicted_index]\n",
    "    print(f\"[{d}] Предсказанное следующее слово для '{''.join([index2word[i] for i in test_indices]).replace('</w>', ' ')}': {predicted_word}\")\n",
    "    \n",
    "    # Вычисляем перплексию для тестового предложения\n",
    "    perplexity = evaluate_perplexity(model, test_sentence, L, word2index)\n",
    "    print(f\"[{d}] Перплексия на тестовом предложении: {perplexity:.2f}\")\n",
    "\n",
    "    generated_words = []\n",
    "    current_sequence = test_indices\n",
    "    for _ in range(100):\n",
    "        predicted = model.predict(np.array([current_sequence]), verbose=0)\n",
    "        predicted_index = np.argmax(predicted)\n",
    "        generated_words.append(index2word[predicted_index])\n",
    "        current_sequence = current_sequence[1:] + [predicted_index]\n",
    "    \n",
    "    if bpe:\n",
    "        s = ''.join(generated_words).split('</w>')\n",
    "        print(f\"Сгенерированные слова: \", *s)\n",
    "    else:\n",
    "        print(f\"Сгенерированные слова: {' '.join(generated_words)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
