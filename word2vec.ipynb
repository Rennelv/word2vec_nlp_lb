{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def embeddings_path(dataset_name, embedding_size, bpe=False):\n",
    "    if bpe:\n",
    "        return f\"embeddings/{dataset_name}_embeddings_{embedding_size}_bpe.txt\"\n",
    "    return f\"embeddings/{dataset_name}_embeddings_{embedding_size}.txt\"\n",
    "\n",
    "def tokenize_text(input_file, output_file, tokenizer, regex, new_line_marker='\\n', max_sentences=None):\n",
    "    sentences = []\n",
    "    with open(input_file, 'r', encoding='utf-8') as fin, \\\n",
    "         open(output_file, 'w', encoding='utf-8') as fout:\n",
    "        pattern = re.compile(regex, re.UNICODE)\n",
    "        for line in fin:\n",
    "            tokens = pattern.findall(line.lower())\n",
    "            if not tokens:\n",
    "                continue\n",
    "            if tokenizer == 'word':\n",
    "                sentences.append(tokens)\n",
    "                sentences[-1].append(new_line_marker + ' ')\n",
    "            elif tokenizer == 'char':\n",
    "                words = []\n",
    "                for token in tokens:\n",
    "                    token_chars = list(token)\n",
    "                    token_chars.append('</w>')\n",
    "                    tokenized_word = ' '.join(token_chars)\n",
    "                    words.append(tokenized_word)\n",
    "                sentences.append(words)\n",
    "                sentences[-1].append(new_line_marker + ' ')\n",
    "            fout.write(\" \".join(sentences[-1]))\n",
    "            if max_sentences is not None and len(sentences) >= max_sentences:\n",
    "                break\n",
    "    return sentences\n",
    "\n",
    "def load_sentences(file_path):\n",
    "    sentences = []\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            tokens = line.strip().split()\n",
    "            sentences.append(tokens)\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words: 6000812\n",
      "Total number of sentences: 1\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"datasets/harrypotter.txt\"\n",
    "# dataset_path = \"datasets/chan_dialogues.txt\"\n",
    "# dataset_path = \"datasets/war.txt\"\n",
    "dataset_name = dataset_path.split('/')[-1].split('.')[0]\n",
    "\n",
    "embeddings_d = [100, 500, 1000]\n",
    "max_vocab_size = 120000\n",
    "\n",
    "bpe = True\n",
    "\n",
    "# pattern = r'\\w+|[^\\w\\s]' # со знаками препинания\n",
    "pattern =  r\"\\w+\"        # только слова   \n",
    "tokenizer = 'char' if bpe else 'word'\n",
    "tokenize_text(dataset_path, f\"sentences/{dataset_name}_sentences_{tokenizer}.txt\", tokenizer=tokenizer, regex=pattern, new_line_marker='</n> ')\n",
    "sentences = load_sentences(f\"sentences/{dataset_name}_sentences_{tokenizer}.txt\")\n",
    "\n",
    "total_words = sum(len(sentence) for sentence in sentences)\n",
    "print(f\"Total number of words: {total_words}\")\n",
    "print(f\"Total number of sentences: {len(sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "if bpe:\n",
    "    subprocess.run([\"./bpe_tokenizer\",\n",
    "                    f\"sentences/{dataset_name}_sentences_{tokenizer}.txt\",\n",
    "                    f\"sentences/{dataset_name}_sentences_bpe.txt\",\n",
    "                   '2800'\n",
    "                   ], check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words after BPE: 1417078\n"
     ]
    }
   ],
   "source": [
    "if bpe:\n",
    "    sentences = load_sentences(f\"sentences/{dataset_name}_sentences_bpe.txt\")\n",
    "    total_words = sum(len(sentence) for sentence in sentences)\n",
    "    print(f\"Total number of words after BPE: {total_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст: sentences/harrypotter_sentences_word.txt\n",
      "Эмбеддинги: embeddings/harrypotter_embeddings_100.txt\n",
      "Размер эмбеддингов (d): 100\n",
      "Макс. размер словаря: 120000\n",
      "Размер окна: 5\n",
      "Количество отрицательных примеров: 5\n",
      "Количество эпох: 5\n",
      "Скорость обучения (lr): 0.025\n",
      "\n",
      "Загрузка текста и создание словаря...\n",
      "Текст прочитан. (0.120547 сек)\n",
      "Количество строк: 1\n",
      "Количество уникальных слов: 22054\n",
      "\n",
      "Словарь создан. (0.128097 сек)\n",
      "Размер словаря: 22054\n",
      "Макс. размер словаря: 120000\n",
      "Примеры словаря: \n",
      "the (52227) -> 0; </n> (38195) -> 1; and (27810) -> 2; to (27129) -> 3; he (22260) -> 4; of (22005) -> 5; a (21094) -> 6; harry (18367) -> 7; was (15689) -> 8; you (14667) -> 9; \n",
      "\n",
      "Предложения закодированы. (0.179729 сек)\n",
      "Количество закодированных предложений: 1\n",
      "Примеры закодированных предложений: \n",
      "the </n> and to he of a harry was you \n",
      "0 1 2 3 4 5 6 7 8 9 \n",
      "\n",
      "Эпоха 1 | Средняя ошибка: 1.15552 | Время: 25.9511 сек | Общее время: 26.2438 сек\n",
      "Эпоха 2 | Средняя ошибка: 0.873556 | Время: 23.0976 сек | Общее время: 49.3414 сек\n",
      "Эпоха 3 | Средняя ошибка: 0.815326 | Время: 23.1292 сек | Общее время: 72.4706 сек\n",
      "Эпоха 4 | Средняя ошибка: 0.778211 | Время: 23.1337 сек | Общее время: 95.6044 сек\n",
      "Эпоха 5 | Средняя ошибка: 0.751762 | Время: 23.1483 сек | Общее время: 118.753 сек\n",
      "Обучение завершено. Эмбеддинги сохранены в embeddings/harrypotter_embeddings_100.txt\n",
      "Время обучения: 119.191 секунд.\n",
      "Текст: sentences/harrypotter_sentences_word.txt\n",
      "Эмбеддинги: embeddings/harrypotter_embeddings_500.txt\n",
      "Размер эмбеддингов (d): 500\n",
      "Макс. размер словаря: 120000\n",
      "Размер окна: 5\n",
      "Количество отрицательных примеров: 5\n",
      "Количество эпох: 5\n",
      "Скорость обучения (lr): 0.025\n",
      "\n",
      "Загрузка текста и создание словаря...\n",
      "Текст прочитан. (0.107516 сек)\n",
      "Количество строк: 1\n",
      "Количество уникальных слов: 22054\n",
      "\n",
      "Словарь создан. (0.114583 сек)\n",
      "Размер словаря: 22054\n",
      "Макс. размер словаря: 120000\n",
      "Примеры словаря: \n",
      "the (52227) -> 0; </n> (38195) -> 1; and (27810) -> 2; to (27129) -> 3; he (22260) -> 4; of (22005) -> 5; a (21094) -> 6; harry (18367) -> 7; was (15689) -> 8; you (14667) -> 9; \n",
      "\n",
      "Предложения закодированы. (0.163798 сек)\n",
      "Количество закодированных предложений: 1\n",
      "Примеры закодированных предложений: \n",
      "the </n> and to he of a harry was you \n",
      "0 1 2 3 4 5 6 7 8 9 \n",
      "\n",
      "Эпоха 1 | Средняя ошибка: 1.15875 | Время: 65.5256 сек | Общее время: 66.2581 сек\n",
      "Эпоха 2 | Средняя ошибка: 0.874404 | Время: 65.0361 сек | Общее время: 131.294 сек\n",
      "Эпоха 3 | Средняя ошибка: 0.816601 | Время: 60.6209 сек | Общее время: 191.915 сек\n",
      "Эпоха 4 | Средняя ошибка: 0.780635 | Время: 58.3005 сек | Общее время: 250.216 сек\n",
      "Эпоха 5 | Средняя ошибка: 0.752737 | Время: 58.2073 сек | Общее время: 308.423 сек\n",
      "Обучение завершено. Эмбеддинги сохранены в embeddings/harrypotter_embeddings_500.txt\n",
      "Время обучения: 310.737 секунд.\n",
      "Текст: sentences/harrypotter_sentences_word.txt\n",
      "Эмбеддинги: embeddings/harrypotter_embeddings_1000.txt\n",
      "Размер эмбеддингов (d): 1000\n",
      "Макс. размер словаря: 120000\n",
      "Размер окна: 5\n",
      "Количество отрицательных примеров: 5\n",
      "Количество эпох: 5\n",
      "Скорость обучения (lr): 0.025\n",
      "\n",
      "Загрузка текста и создание словаря...\n",
      "Текст прочитан. (0.104569 сек)\n",
      "Количество строк: 1\n",
      "Количество уникальных слов: 22054\n",
      "\n",
      "Словарь создан. (0.111372 сек)\n",
      "Размер словаря: 22054\n",
      "Макс. размер словаря: 120000\n",
      "Примеры словаря: \n",
      "the (52227) -> 0; </n> (38195) -> 1; and (27810) -> 2; to (27129) -> 3; he (22260) -> 4; of (22005) -> 5; a (21094) -> 6; harry (18367) -> 7; was (15689) -> 8; you (14667) -> 9; \n",
      "\n",
      "Предложения закодированы. (0.153321 сек)\n",
      "Количество закодированных предложений: 1\n",
      "Примеры закодированных предложений: \n",
      "the </n> and to he of a harry was you \n",
      "0 1 2 3 4 5 6 7 8 9 \n",
      "\n",
      "Эпоха 1 | Средняя ошибка: 1.16017 | Время: 99.7252 сек | Общее время: 100.935 сек\n",
      "Эпоха 2 | Средняя ошибка: 0.874555 | Время: 103.012 сек | Общее время: 203.946 сек\n",
      "Эпоха 3 | Средняя ошибка: 0.816938 | Время: 112.546 сек | Общее время: 316.492 сек\n",
      "Эпоха 4 | Средняя ошибка: 0.781279 | Время: 115.487 сек | Общее время: 431.98 сек\n",
      "Эпоха 5 | Средняя ошибка: 0.754392 | Время: 112.859 сек | Общее время: 544.838 сек\n",
      "Обучение завершено. Эмбеддинги сохранены в embeddings/harrypotter_embeddings_1000.txt\n",
      "Время обучения: 549.704 секунд.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "for d in embeddings_d:\n",
    "    subprocess.run([\"./generate_embeddings\",            # executable\n",
    "                    f\"sentences/{dataset_name}_sentences_{'bpe' if bpe else 'word'}.txt\",       # path to tokenized sentences\n",
    "                    embeddings_path(dataset_name, d, bpe=bpe),   # path to save embeddings to\n",
    "                    str(d),                             # word vec dim\n",
    "                    str(max_vocab_size),                # max size of vocabulary\n",
    "                    '5',                                # windows size\n",
    "                    '5',                                # num negative examples\n",
    "                    '5',                                # num epoch\n",
    "                    '0.025'                             # learning rate\n",
    "                    ], check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_embeddings(file_path):\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        header = f.readline().strip().split()\n",
    "        vocab_size = int(header[0])\n",
    "        d = int(header[1])\n",
    "        embeddings = np.zeros((vocab_size, d), dtype=np.float32)\n",
    "        word2index = {}\n",
    "        index2word = []\n",
    "        for i, line in enumerate(f):\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) != d + 1:\n",
    "                continue \n",
    "            word = parts[0]\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            embeddings[i] = vector\n",
    "            word2index[word] = i\n",
    "            index2word.append(word)\n",
    "    return embeddings, word2index, index2word\n",
    "\n",
    "# embeddings_file = embeddings_path(dataset_name, 100)  \n",
    "# embeddings, word2index, index2word = load_embeddings(embeddings_file)\n",
    "# print(\"Загруженная матрица эмбеддингов:\", embeddings.shape)\n",
    "# print(\"Примеры слов:\", list(word2index.keys())[:10])\n",
    "\n",
    "# print(\"Примеры векторов:\", list(embeddings)[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_examples_from_sentence(sentence, L):\n",
    "    \"\"\"\n",
    "    Генерирует обучающие примеры (контекст, целевой токен) из предложения.\n",
    "    где контекст — это L последовательных токенов, а целевой токен — следующий за ними.\n",
    "    \"\"\"\n",
    "    if len(sentence) < L + 1:\n",
    "        return  # Недостаточно токенов для формирования хотя бы одного примера.\n",
    "    for i in range(len(sentence) - L):\n",
    "        context = sentence[i : i + L]\n",
    "        target = sentence[i + L]\n",
    "        yield context, target\n",
    "        \n",
    "def batch_generator_emb(tokenized_sentences, word2index, embeddings, L, batch_size):\n",
    "    while True:\n",
    "        X, y = [], []\n",
    "        for sentence in tokenized_sentences:\n",
    "            for context, target in generate_training_examples_from_sentence(sentence, L):\n",
    "                X.append([word2index.get(w, 0) for w in context])\n",
    "                y.append(word2index.get(target, 0))\n",
    "                if len(X) >= batch_size:\n",
    "                    X_emb = embeddings[X]\n",
    "                    yield X_emb, np.array(y, dtype=np.int32)\n",
    "                    X, y = [], []  # Очищаем батч\n",
    "        if X:  # Возвращаем остаток батча\n",
    "            X_emb = embeddings[X]\n",
    "            yield X_emb, np.array(y, dtype=np.int32)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер словаря: 2813, размер эмбеддинга: 100\n",
      "Всего слов: 1417078, шагов на эпоху: 2768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742661839.473690  657034 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1216 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">250,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2813</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,409,313</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m)            │       \u001b[38;5;34m250,500\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2813\u001b[0m)           │     \u001b[38;5;34m1,409,313\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,659,813</span> (6.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,659,813\u001b[0m (6.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,659,813</span> (6.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,659,813\u001b[0m (6.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1742661840.536259  657265 service.cc:152] XLA service 0x7e02dc004700 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1742661840.536297  657265 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2025-03-22 19:44:00.556298: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1742661840.637674  657265 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-03-22 19:44:01.572318: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_102', 108 bytes spill stores, 108 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:02.070296: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_102', 320 bytes spill stores, 320 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:02.247600: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_102', 216 bytes spill stores, 216 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:02.291949: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_220', 1328 bytes spill stores, 1032 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:02.307883: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_102', 432 bytes spill stores, 432 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:02.518701: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_102', 1632 bytes spill stores, 1632 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:02.577857: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_102', 160 bytes spill stores, 160 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:02.732449: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_102', 308 bytes spill stores, 308 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:03.251674: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_102', 416 bytes spill stores, 416 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:03.486411: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_102', 116 bytes spill stores, 116 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:03.552563: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_102', 1588 bytes spill stores, 1596 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:03.607140: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_102', 476 bytes spill stores, 476 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:03.675236: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_220', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:03.979109: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_102', 292 bytes spill stores, 292 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:04.191841: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_220', 432 bytes spill stores, 432 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:04.404558: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_102', 108 bytes spill stores, 108 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:04.513157: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_102', 548 bytes spill stores, 548 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:04.536292: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_234', 316 bytes spill stores, 316 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:05.018727: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_220_0', 536 bytes spill stores, 536 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:05.403931: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_220', 676 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:05.560954: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_234', 1532 bytes spill stores, 1532 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:05.602123: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_220', 8520 bytes spill stores, 8580 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:05.803037: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_220_0', 556 bytes spill stores, 456 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:06.316418: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_220', 48 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:06.498544: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_220', 804 bytes spill stores, 648 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:06.522339: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_234', 532 bytes spill stores, 532 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:06.616055: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_234', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:06.893389: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_234', 92 bytes spill stores, 92 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:07.070701: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_220', 252 bytes spill stores, 252 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:07.071852: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_220', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:07.172110: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_234', 80 bytes spill stores, 80 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:07.530336: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_220', 1856 bytes spill stores, 1388 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:07.662422: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_234', 220 bytes spill stores, 220 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:07.797799: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_220', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:07.945963: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_220', 192 bytes spill stores, 192 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:08.175944: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_234', 456 bytes spill stores, 456 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:08.316951: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_234', 1852 bytes spill stores, 1852 bytes spill loads\n",
      "\n",
      "2025-03-22 19:44:08.561539: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_234', 1208 bytes spill stores, 1208 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  45/2768\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.0240 - loss: 7.2622      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742661849.625873  657265 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1901/2768\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.0714 - loss: 6.1717"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     35\u001b[39m model.summary()\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Обучаем модель предсказания\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_generator_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword2index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mloss\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Сохраняем модель\u001b[39;00m\n\u001b[32m     44\u001b[39m model.save(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodels/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00md\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33mbpe\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mbpe\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mword\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_emb.keras\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/etu/ai/.conda/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/etu/ai/.conda/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    370\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/etu/ai/.conda/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:220\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    217\u001b[39m     iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    218\u001b[39m ):\n\u001b[32m    219\u001b[39m     opt_outputs = multi_step_on_iterator(iterator)\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mopt_outputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhas_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m opt_outputs.get_value()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/etu/ai/.conda/lib/python3.11/site-packages/tensorflow/python/data/ops/optional_ops.py:176\u001b[39m, in \u001b[36m_OptionalImpl.has_value\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhas_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, name=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    175\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ops.colocate_with(\u001b[38;5;28mself\u001b[39m._variant_tensor):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_optional_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptional_has_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variant_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/etu/ai/.conda/lib/python3.11/site-packages/tensorflow/python/ops/gen_optional_ops.py:172\u001b[39m, in \u001b[36moptional_has_value\u001b[39m\u001b[34m(optional, name)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tld.is_eager:\n\u001b[32m    171\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     _result = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mOptionalHasValue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m    175\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.backend import clear_session as clear_keras_session\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "L = 5    # Длина контекста для предсказания\n",
    "dh = 500  # Размер скрытого слоя\n",
    "epochs = 10\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "steps_per_epoch = np.ceil(total_words / batch_size).astype(int)\n",
    "\n",
    "for d in embeddings_d:\n",
    "    embeddings_file = embeddings_path(dataset_name, d, bpe=bpe)\n",
    "    embeddings, word2index, index2word = load_embeddings(embeddings_file)\n",
    "\n",
    "    vocab_size = len(word2index)\n",
    "\n",
    "    print(f\"Размер словаря: {vocab_size}, размер эмбеддинга: {d}\")\n",
    "    print(f\"Всего слов: {total_words}, шагов на эпоху: {steps_per_epoch}\")\n",
    "\n",
    "    clear_keras_session()  # Очистка сессии Keras\n",
    "\n",
    "    model = Sequential([ \n",
    "        layers.Input(shape=(L,d)), # \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(dh, activation='relu'),\n",
    "        layers.Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    # Обучаем модель предсказания\n",
    "    model.fit(batch_generator_emb(sentences, word2index, embeddings, L, batch_size), \n",
    "            epochs=epochs, \n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            callbacks=[EarlyStopping(monitor='loss', patience=3)])\n",
    "    \n",
    "    # Сохраняем модель\n",
    "    model.save(f\"models/{dataset_name}_model_{d}_{'bpe' if bpe else 'word'}_emb.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тестовое предложение: 'ters</w> mind</w> you</w> pl enty</w> of</w> people</w> thought</w> he</w> was</w> going</w> about</w> things</w> the</w> right</w> way</w>'\n",
      "Тестовые индексы: [69, 612, 0, 144, 171]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271ms/step\n",
      "[100] Предсказанное следующее слово для 'about things the right way ': to</w>\n",
      "[100] Перплексия на тестовом предложении: 87.24\n",
      "Сгенерированные слова:  to be sure that he had never seen it was a good idea of the wand and the wand and harry saw that he had never seen it was a good idea of the wand and the wand and harry saw that he had never seen it was a good idea of the wand and the wand and harry saw that he had never seen it was a good idea of the wand and the wand and harry saw that he had never seen it was a good idea of the wand and the wand and harry saw that he \n",
      "Тестовые индексы: [69, 612, 0, 144, 171]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287ms/step\n",
      "[500] Предсказанное следующее слово для 'about things the right way ': the</w>\n",
      "[500] Перплексия на тестовом предложении: 72.43\n",
      "Сгенерированные слова:  the door of the hall and the deathly hallows </n>chapter thirty six </n>the prince blood and the deathly hallows </n>chapter thirty six </n>the prince blood and the deathly hallows </n>chapter thirty six </n>the prince blood and the deathly hallows </n>chapter thirty six </n>the prince blood and the deathly hallows </n>chapter thirty six </n>the prince blood and the deathly hallows </n>chapter thirty six </n>the prince blood and the deathly hallows </n>chapter thirty six </n>the prince blood and the deathly hallows </n>chapter thirty six </n>the prince \n",
      "Тестовые индексы: [69, 612, 0, 144, 171]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step\n",
      "[1000] Предсказанное следующее слово для 'about things the right way ': of</w>\n",
      "[1000] Перплексия на тестовом предложении: 81.45\n",
      "Сгенерированные слова:  of the way of the wand and the cloak and he was still watching the room and the same way and the death eaters were still more than ever he had been a long time he was not going to be a bit of time the wand of the wand and the cloak and he was still watching the room and the same way and the death eaters were still more than ever he had been a long time he was not going to be a bit of time the wand of the wand and the cloak and he was \n"
     ]
    }
   ],
   "source": [
    "# self embeddings layer\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.backend import clear_session as clear_keras_session\n",
    "\n",
    "def evaluate_perplexity(model, test_sentence, L, word2index, embeddings):\n",
    "    \"\"\"\n",
    "    Для каждого окна длины L модель предсказывает вероятность истинного следующего слова\n",
    "    perplexity = exp( -(1/N) * sum(log(P(w_i|context)))\n",
    "    где N - количество предсказаний, а P(w_i|context) - вероятность истинного слова\n",
    "    \"\"\"\n",
    "    total_log_prob = 0.0\n",
    "    count = 0\n",
    "\n",
    "    test_tokens = test_sentence.lower().split()\n",
    "    test_indices = [word2index.get(word, 0) for word in test_tokens]\n",
    "    \n",
    "    test_indices_emb = embeddings[test_indices]\n",
    "    \n",
    "    # Если тестовая последовательность слишком короткая возвращаем бесконечность\n",
    "    if len(test_indices_emb) <= L:\n",
    "        return float('inf')\n",
    "\n",
    "    # Проходим по тестовой последовательности, формируя окна длины L\n",
    "    for i in range(len(test_indices_emb) - L):\n",
    "        context = test_indices_emb[i:i+L]         # Контекстное окно из L токенов\n",
    "        true_word = test_indices[i+L]           # Истинное следующее слово\n",
    "        \n",
    "        # Получаем распределение вероятностей для следующего слова\n",
    "        pred = model.predict(np.array([context]), verbose=0)[0]\n",
    "        # Берем вероятность истинного слова; добавляем маленькую константу для стабильности log\n",
    "        prob = pred[true_word] + 1e-10\n",
    "        \n",
    "        # Суммируем логарифмы вероятностей\n",
    "        total_log_prob += np.log(prob)\n",
    "        count += 1\n",
    "\n",
    "    if count == 0:\n",
    "        return float('inf')\n",
    "        \n",
    "    # Вычисляем среднее логарифмическое значение\n",
    "    avg_log_prob = total_log_prob / count\n",
    "    # Перплексия – экспонента от отрицательной средней логарифмической вероятности\n",
    "    perplexity = np.exp(-avg_log_prob)\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "# Длина контекстного окна для предсказания следующего слова\n",
    "L = 5  \n",
    "# Длина тестового предложения\n",
    "S = 15\n",
    "# Формируем один длинный список из всех слов из файла (load_sentences должна возвращать список предложений, где каждое предложение - список токенов)\n",
    "sentences_unpacked = []\n",
    "for sentence in load_sentences(f\"sentences/{dataset_name}_sentences_{'bpe' if bpe else 'word'}.txt\"):\n",
    "    sentences_unpacked.extend(sentence) \n",
    "\n",
    "# Выбираем случайное окно в общем списке\n",
    "i = random.randint(0, len(sentences_unpacked) - S - 1)\n",
    "# Берем окно из L+1 слов (последнее слово будет истинным следом за контекстом)\n",
    "test_sentence = \" \".join(sentences_unpacked[i:i+S+1])\n",
    "# test_sentence = 'my</w> name</w> is</w>'\n",
    "print(f\"Тестовое предложение: '{test_sentence}'\")\n",
    "\n",
    "# Преобразуем предложение в список токенов (слова приводятся к нижнему регистру)\n",
    "test_tokens = test_sentence.lower().split()\n",
    "\n",
    "# Если число токенов меньше L, дополняем их пробелами; если больше, берем последние L токенов.\n",
    "if len(test_tokens) < L:\n",
    "    test_tokens = [' '] * (L - len(test_tokens)) + test_tokens\n",
    "elif len(test_tokens) > L:\n",
    "    test_tokens = test_tokens[-L:]\n",
    "\n",
    "# Для каждого размера эмбеддингов оцениваем модель\n",
    "for d in embeddings_d:\n",
    "    # Загружаем модель, соответствующую текущей размерности эмбеддингов\n",
    "    clear_keras_session()  # Очистка сессии Keras\n",
    "    model = load_model(f\"models/{dataset_name}_model_{d}_{'bpe' if bpe else 'word'}_emb.keras\")\n",
    "    \n",
    "    # Загружаем эмбеддинги и словари\n",
    "    embeddings_file = embeddings_path(dataset_name, d, bpe=bpe)\n",
    "    embeddings, word2index, index2word = load_embeddings(embeddings_file)\n",
    "\n",
    "    # Преобразуем токены в индексы\n",
    "    test_indices = [word2index.get(word, 0) for word in test_tokens]\n",
    "    test_indices_emb = embeddings[test_indices]\n",
    "    \n",
    "    print(f\"Тестовые индексы: {test_indices}\")\n",
    "    \n",
    "    \n",
    "    # Предсказываем следующее слово по текущему контексту\n",
    "    predicted = model.predict(np.array([test_indices_emb]))\n",
    "    predicted_index = np.argmax(predicted)\n",
    "    predicted_word = index2word[predicted_index]\n",
    "    print(f\"[{d}] Предсказанное следующее слово для '{' '.join(test_tokens).replace(' ' if bpe else '', '').replace('</w>', ' ')}': {predicted_word}\")\n",
    "    \n",
    "    # Вычисляем перплексию для тестового предложения\n",
    "    perplexity = evaluate_perplexity(model, test_sentence, L, word2index, embeddings)\n",
    "    print(f\"[{d}] Перплексия на тестовом предложении: {perplexity:.2f}\")\n",
    "\n",
    "    generated_words = []\n",
    "    current_sequence = embeddings[test_indices]\n",
    "    for _ in range(100):\n",
    "        predicted = model.predict(np.array([current_sequence]), verbose=0)\n",
    "        predicted_index = np.argmax(predicted)\n",
    "        generated_words.append(index2word[predicted_index])\n",
    "        current_sequence = np.vstack((current_sequence[1:], embeddings[predicted_index]))\n",
    "    \n",
    "    if bpe:\n",
    "        s = ''.join(generated_words).split('</w>')\n",
    "        print(f\"Сгенерированные слова: \", *s)\n",
    "    else:\n",
    "        print(f\"Сгенерированные слова: {' '.join(generated_words)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
